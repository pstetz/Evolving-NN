{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "from sklearn import datasets\n",
    "\n",
    "def relu(X):\n",
    "    return np.maximum(X, 0)\n",
    "\n",
    "def relu_derivative(X):\n",
    "    return 1 * (X > 0)\n",
    "\n",
    "def sum_square(arr):\n",
    "    return np.sum(np.square(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "Taken from [here](https://github.com/jldbc/numpy_neural_net/blob/master/three_layer_network.py) with some modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, shapes=None, epsilon=0.12):\n",
    "        assert shapes, \"Please provide the shape of the NN\"\n",
    "        \n",
    "        self.num_layers = len(shapes) - 1\n",
    "        self.w = []\n",
    "        self.b = []\n",
    "        for i in range(self.num_layers):\n",
    "            shape = (shapes[i], shapes[i+1])\n",
    "            \n",
    "            w = np.random.uniform(-epsilon, epsilon, shape)\n",
    "            b = np.zeros((1, shape[1]))\n",
    "            \n",
    "            self.w.append(w)\n",
    "            self.b.append(b)\n",
    "\n",
    "    def feed_forward(self, x):\n",
    "        a = []\n",
    "        for i in range(self.num_layers - 1):\n",
    "            w = self.w[i]\n",
    "            b = self.b[i]\n",
    "            \n",
    "            x = x.dot(w) + b\n",
    "            a.append(relu(x))\n",
    "        \n",
    "        z = a[-1].dot(self.w[-1]) + self.b[-1]\n",
    "        z = np.exp(z)\n",
    "        \n",
    "        out = z / np.sum(z, axis=1, keepdims=True)\n",
    "        return a[0], out\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        num_examples = X.shape[0]\n",
    "        \n",
    "        # Forward propagation to calculate our predictions\n",
    "        _, out = self.feed_forward(X)\n",
    "        probs  = out / np.sum(out, axis=1, keepdims=True)\n",
    "        \n",
    "        # Calculating the loss\n",
    "        corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "        loss = np.sum(corect_logprobs)\n",
    "        \n",
    "        # Add regulatization term to loss (optional)\n",
    "        loss += self.reg_lambda/2 * sum([sum_square(w) for w in self.w])\n",
    "        return 1./num_examples * loss\n",
    "\n",
    "    def backprop(self, X, y, a1, output):\n",
    "        dW, db = [], []\n",
    "        \n",
    "        delta = output\n",
    "        delta[range(X.shape[0]), y] -= 1\n",
    "        \n",
    "        dW = [(a1.T).dot(delta)]     + dW\n",
    "        db = [np.sum(delta, axis=0)] + db\n",
    "\n",
    "        delta = delta.dot(self.w[1].T) * relu_derivative(a1)\n",
    "        dW = [np.dot(X.T, delta)]    + dW\n",
    "        db = [np.sum(delta, axis=0)] + db\n",
    "        \n",
    "        # Add regularization terms\n",
    "        for i in range(self.num_layers):\n",
    "            dW[i] += self.reg_lambda * self.w[i]\n",
    "        \n",
    "        #update weights and biases\n",
    "        for i in range(self.num_layers):\n",
    "            self.w[i] -= self.learning_rate * dW[i]\n",
    "            self.b[i] -= self.learning_rate * db[i]\n",
    "\n",
    "\n",
    "    def train(self, X, y, epochs=10000, reg_lambda = 0.1, learning_rate=0.1, verbose=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda    = reg_lambda\n",
    "        \n",
    "        # Batch gradient descent\n",
    "        losses = []\n",
    "        for i in range(epochs):\n",
    "            \n",
    "            #feed forward\n",
    "            a1, output = self.feed_forward(X)\n",
    "            \n",
    "            #backpropagation\n",
    "            self.backprop(X, y, a1, output)\n",
    "            \n",
    "            if i % verbose == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                losses.append(loss)\n",
    "                print(f\"Loss after iteration {i}: {loss}\")\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0: 0.6810301186100619\n",
      "Loss after iteration 10000: 0.06212428849314495\n",
      "Loss after iteration 20000: 0.01554714278965735\n",
      "Loss after iteration 30000: 0.009528478262497558\n",
      "Loss after iteration 40000: 0.007564513297852371\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = datasets.make_moons(16, noise=0.10)\n",
    "shapes = [train_x.shape[1], 150, 2]\n",
    "model = NN(shapes=shapes)\n",
    "model.train(train_x, train_y, reg_lambda=0.001, learning_rate=0.001, epochs=50000, verbose=10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
